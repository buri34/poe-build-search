"""YouTube ãƒ“ãƒ«ãƒ‰ã‚¬ã‚¤ãƒ‰å‹•ç”»ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼

YouTubeå‹•ç”»ã®æ¤œç´¢ãƒ»ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ»å­—å¹•å–å¾—ãƒ»LLMæŠ½å‡ºãƒ»DBæ ¼ç´ã‚’ä¸€è²«ã—ã¦è¡Œã†ã€‚
"""
import asyncio
import json
import math
import os
import re
import subprocess
from datetime import datetime, timedelta, timezone
from pathlib import Path

from youtube_transcript_api import YouTubeTranscriptApi
import yt_dlp

from scraper.base import save_builds_to_db, detect_combat_style, detect_specialty
from scraper.llm_extractor import extract_build_info_via_llm


# æ¤œç´¢ã‚¯ã‚¨ãƒª
SEARCH_QUERIES = [
    "PoE 3.27 build guide",
    "Path of Exile 3.27 build",
    "PoE Keepers of the Flame build guide",
    "PoE 3.27 league starter",
    "PoE 3.27 starter build guide",
]

# äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
MIN_DURATION_SECONDS = 300  # 5åˆ†
MAX_AGE_DAYS = 180  # 6ãƒ¶æœˆ


def search_youtube_videos() -> list[dict]:
    """YouTubeå‹•ç”»ã‚’æ¤œç´¢ã—ã€é‡è¤‡æ’é™¤ãƒ»äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨"""
    print("=" * 60)
    print("STEP 1: YouTubeå‹•ç”»æ¤œç´¢")
    print("=" * 60)

    all_videos = {}  # video_id -> video_data

    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': True,
        'skip_download': True,
    }

    for query in SEARCH_QUERIES:
        print(f"\nğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")
        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                # ytsearch20: ã§20ä»¶å–å¾—
                search_results = ydl.extract_info(f"ytsearch20:{query}", download=False)

                if not search_results or 'entries' not in search_results:
                    print(f"  æ¤œç´¢çµæœãªã—")
                    continue

                for video in search_results['entries']:
                    if not video:
                        continue

                    video_id = video.get('id')
                    if not video_id:
                        continue

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if video_id in all_videos:
                        continue

                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                    duration_seconds = video.get('duration', 0)

                    # æŠ•ç¨¿æ—¥ï¼ˆunixã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¾ãŸã¯æ–‡å­—åˆ—ï¼‰
                    timestamp = video.get('timestamp')
                    if timestamp:
                        published_date = datetime.fromtimestamp(timestamp, tz=timezone.utc)
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç¾åœ¨æ™‚åˆ»
                        published_date = datetime.now(timezone.utc)

                    # äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿: 5åˆ†æœªæº€é™¤å¤–
                    if duration_seconds < MIN_DURATION_SECONDS:
                        continue

                    # äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿: 6ãƒ¶æœˆä»¥å†…ã®ã¿
                    days_ago = (datetime.now(timezone.utc) - published_date).days
                    if days_ago > MAX_AGE_DAYS:
                        continue

                    # ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²è€…æ•°
                    subscriber_count = video.get('channel_follower_count', 0) or 0

                    # è¦–è´å›æ•°
                    view_count = video.get('view_count', 0) or 0

                    video_data = {
                        'video_id': video_id,
                        'title': video.get('title', ''),
                        'channel_name': video.get('channel', '') or video.get('uploader', ''),
                        'channel_subscriber_count': subscriber_count,
                        'view_count': view_count,
                        'published_date': published_date,
                        'duration_seconds': duration_seconds,
                        'video_url': f"https://www.youtube.com/watch?v={video_id}",
                        'thumbnail': video.get('thumbnail', ''),
                    }

                    all_videos[video_id] = video_data

                print(f"  ãƒ’ãƒƒãƒˆ: {len(search_results['entries'])}ä»¶, ãƒ•ã‚£ãƒ«ã‚¿å¾Œè¿½åŠ : {len(all_videos)}ä»¶ï¼ˆç´¯è¨ˆï¼‰")

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆåŒæœŸçš„ã«sleepï¼‰
            import time
            time.sleep(1.5)

        except Exception as e:
            print(f"  âš ï¸ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            continue

    print(f"\nâœ… æ¤œç´¢å®Œäº†: {len(all_videos)}ä»¶ã®å‹•ç”»ã‚’å–å¾—ï¼ˆé‡è¤‡æ’é™¤ãƒ»äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
    return list(all_videos.values())


def calculate_metadata_score(video: dict) -> float:
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    now = datetime.now(timezone.utc)
    published_date = video['published_date']

    days_since_publish = max((now - published_date).days, 1)
    view_count = video['view_count']
    subscriber_count = video['channel_subscriber_count']

    # ç°¡æ˜“çš„ã«like/commentæ•°ã‚’æ¨å®šï¼ˆå®Ÿéš›ã®APIã§ã¯å–å¾—å¯èƒ½ï¼‰
    # ã“ã“ã§ã¯ view_count ã® 5% ã‚’like, 0.5% ã‚’commentã¨ä»®å®š
    like_count = int(view_count * 0.05)
    comment_count = int(view_count * 0.005)

    # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    view_velocity = view_count / days_since_publish
    like_ratio = like_count / max(view_count, 1)
    comment_density = comment_count / max(view_count, 1)
    channel_factor = math.log10(max(subscriber_count, 1)) / 7

    score = (
        view_velocity * 0.35 +
        like_ratio * 0.25 * 10000 +
        comment_density * 0.20 * 10000 +
        channel_factor * 0.20 * 1000
    )

    # æŠ•ç¨¿7æ—¥ä»¥å†…ãƒœãƒ¼ãƒŠã‚¹
    if days_since_publish <= 7:
        score += 500

    return score


def score_and_filter_videos(videos: list[dict], top_n: int = 50) -> list[dict]:
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã¦ä¸Šä½Nä»¶ã‚’æŠ½å‡º"""
    print("\n" + "=" * 60)
    print("STEP 2: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° + ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°")
    print("=" * 60)

    # è¤‡æ•°ãƒ“ãƒ«ãƒ‰ç´¹ä»‹å‹•ç”»ã‚’é™¤å¤–ï¼ˆTier List, Top 10, Best Builds ãªã©ï¼‰
    exclude_keywords = [
        "tier list", "top 10", "top 5", "top tier", "best builds",
        "best starters", "best league", "ranking", "flowchart"
    ]

    filtered_videos = []
    excluded_count = 0
    for video in videos:
        title_lower = video['title'].lower()
        if any(kw in title_lower for kw in exclude_keywords):
            excluded_count += 1
            continue
        filtered_videos.append(video)

    print(f"  é™¤å¤–: {excluded_count}ä»¶ï¼ˆè¤‡æ•°ãƒ“ãƒ«ãƒ‰ç´¹ä»‹å‹•ç”»ï¼‰")
    print(f"  æ®‹ã‚Š: {len(filtered_videos)}ä»¶")

    for video in filtered_videos:
        video['metadata_score'] = calculate_metadata_score(video)

    # ã‚¹ã‚³ã‚¢é™é †ã§ã‚½ãƒ¼ãƒˆ
    filtered_videos.sort(key=lambda v: v['metadata_score'], reverse=True)

    print(f"\nğŸ“Š ã‚¹ã‚³ã‚¢åˆ†å¸ƒï¼ˆä¸Šä½10ä»¶ï¼‰:")
    for i, video in enumerate(filtered_videos[:10], 1):
        days_ago = (datetime.now(timezone.utc) - video['published_date']).days
        print(f"  {i}. {video['title'][:50]}... (ã‚¹ã‚³ã‚¢: {video['metadata_score']:.1f}, {days_ago}æ—¥å‰, {video['view_count']:,} views)")

    top_videos = filtered_videos[:top_n]
    print(f"\nâœ… ä¸Šä½{top_n}ä»¶ã‚’é¸æŠœ")

    return top_videos


async def get_video_transcript(video_id: str) -> str | None:
    """å‹•ç”»ã®å­—å¹•ã‚’å–å¾—ï¼ˆè‹±èªå„ªå…ˆï¼‰"""
    try:
        # æ–°ã—ã„APIã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        api = YouTubeTranscriptApi()
        # è‹±èªå­—å¹•ã‚’å–å¾—ï¼ˆæ‰‹å‹•ã¾ãŸã¯è‡ªå‹•ç”Ÿæˆï¼‰
        fetched = api.fetch(video_id, languages=['en'])

        # FetchedTranscript ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ snippets ã‚’å–å¾—
        full_text = ' '.join([snippet.text for snippet in fetched.snippets])

        # å…ˆé ­15000æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚
        if len(full_text) > 15000:
            full_text = full_text[:15000]

        return full_text

    except Exception as e:
        # å­—å¹•ãªã— or ã‚¨ãƒ©ãƒ¼ï¼ˆè©³ç´°ãƒ­ã‚°å‡ºåŠ›ï¼‰
        print(f"    å­—å¹•å–å¾—ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)[:100]}")
        return None


async def extract_build_from_transcript(video: dict, transcript: str) -> dict | None:
    """å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ“ãƒ«ãƒ‰æƒ…å ±ã‚’LLMæŠ½å‡º"""
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ æŒ‡ç¤ºã‚’å«ã‚ã¦æ—¢å­˜ã®LLMæŠ½å‡ºã‚’å‘¼ã³å‡ºã™
    enhanced_prompt = f"""ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯YouTubeå‹•ç”»ã®æ›¸ãèµ·ã“ã—ã§ã™ã€‚
ãƒ•ã‚£ãƒ©ãƒ¼ï¼ˆuh, umç­‰ï¼‰ã¯ç„¡è¦–ã—ã€ãƒ“ãƒ«ãƒ‰æƒ…å ±ã®ã¿æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
è¤‡æ•°ãƒ“ãƒ«ãƒ‰ç´¹ä»‹æ™‚ã¯ãƒ¡ã‚¤ãƒ³ãƒ“ãƒ«ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{transcript}"""

    # æ—¢å­˜ã®extract_build_info_via_llmã‚’ä½¿ç”¨
    result = extract_build_info_via_llm(enhanced_prompt, video['title'])

    if not result or not result.get('description_en'):
        return None

    # ãƒ“ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
    build = {
        'source': 'youtube',
        'source_id': video['video_id'],
        'source_url': video['video_url'],
        'name_en': video['title'],  # LLMæŠ½å‡ºçµæœãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
        'class_en': result.get('class_en') or 'Unknown',
        'ascendancy_en': result.get('ascendancy_en'),
        'skills_en': json.dumps([]),  # LLMã‹ã‚‰æŠ½å‡ºã—ãŸã‚¹ã‚­ãƒ«ãŒã‚ã‚Œã°è¿½åŠ å¯èƒ½
        'description_en': result.get('description_en'),
        'patch': '3.27',
        'build_types': json.dumps([]),
        'author': video['channel_name'],
        'favorites': video['view_count'],
        'verified': 0,
        'hc': 0,
        'ssf': 0,
        'playstyle': None,
        'activities': None,
        'cost_tier': None,
        'damage_types': None,
        'combat_style': detect_combat_style(
            video['title'],
            [],
            result.get('description_en', '')
        ),
        'specialty': json.dumps(detect_specialty([], result.get('description_en', ''))),
        'pros_cons_en': result.get('pros_cons_en'),
        'pros_cons_ja': None,
        'core_equipment_en': result.get('core_equipment_en'),
        'core_equipment_ja': None,
    }

    return build


async def scrape_youtube_builds():
    """YouTubeãƒ“ãƒ«ãƒ‰ã‚¬ã‚¤ãƒ‰å‹•ç”»ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å…¨ä½“ãƒ•ãƒ­ãƒ¼"""
    print("\n" + "=" * 60)
    print("YouTubeãƒ“ãƒ«ãƒ‰ã‚¬ã‚¤ãƒ‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ é–‹å§‹")
    print("=" * 60)

    # STEP 1: å‹•ç”»æ¤œç´¢
    videos = search_youtube_videos()

    if not videos:
        print("âŒ æ¤œç´¢çµæœãŒ0ä»¶ã§ã™")
        return

    # STEP 2: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆä¸Šä½50ä»¶ï¼‰
    top_videos = score_and_filter_videos(videos, top_n=50)

    # STEP 3: ã‚³ãƒ¡ãƒ³ãƒˆã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ¤å®šã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæŠ€è¡“çš„å›°é›£æ€§ï¼‰
    print("\n" + "=" * 60)
    print("STEP 3: ã‚³ãƒ¡ãƒ³ãƒˆã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ¤å®š")
    print("=" * 60)
    print("âš ï¸ ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—APIã®åˆ¶é™ã«ã‚ˆã‚Šã€ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
    print("   ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢ã®ã¿ã§ä¸Šä½30ä»¶ã‚’é¸æŠœã—ã¾ã™")

    selected_videos = top_videos[:30]

    # STEP 4-5: å­—å¹•å–å¾— & LLMæŠ½å‡º
    print("\n" + "=" * 60)
    print("STEP 4-5: å­—å¹•å–å¾— & LLMæŠ½å‡º")
    print("=" * 60)

    builds = []
    skipped_videos = []

    for i, video in enumerate(selected_videos, 1):
        print(f"\n[{i}/{len(selected_videos)}] {video['title'][:60]}...")

        # å­—å¹•å–å¾—
        transcript = await get_video_transcript(video['video_id'])

        if not transcript:
            print(f"  âš ï¸ å­—å¹•å–å¾—å¤±æ•— - ã‚¹ã‚­ãƒƒãƒ—")
            skipped_videos.append({
                'video_id': video['video_id'],
                'title': video['title'],
                'reason': 'å­—å¹•ãªã—'
            })
            continue

        print(f"  âœ… å­—å¹•å–å¾—æˆåŠŸ ({len(transcript)}æ–‡å­—)")

        # LLMæŠ½å‡º
        build = await extract_build_from_transcript(video, transcript)

        if not build:
            print(f"  âš ï¸ LLMæŠ½å‡ºå¤±æ•— - ã‚¹ã‚­ãƒƒãƒ—")
            skipped_videos.append({
                'video_id': video['video_id'],
                'title': video['title'],
                'reason': 'LLMæŠ½å‡ºå¤±æ•—'
            })
            continue

        print(f"  âœ… LLMæŠ½å‡ºæˆåŠŸ")
        builds.append(build)

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        await asyncio.sleep(2)

    # STEP 6: DBæ ¼ç´
    print("\n" + "=" * 60)
    print("STEP 6: DBæ ¼ç´")
    print("=" * 60)

    if builds:
        await save_builds_to_db(builds)
        print(f"\nâœ… DBæ ¼ç´å®Œäº†: {len(builds)}ä»¶")
    else:
        print("âš ï¸ æ ¼ç´ã™ã‚‹ãƒ“ãƒ«ãƒ‰ãŒ0ä»¶ã§ã™")

    # ã‚¹ã‚­ãƒƒãƒ—ã—ãŸå‹•ç”»ã®ã‚µãƒãƒªãƒ¼
    if skipped_videos:
        print(f"\nâš ï¸ ã‚¹ã‚­ãƒƒãƒ—ã—ãŸå‹•ç”»: {len(skipped_videos)}ä»¶")
        for sv in skipped_videos[:5]:
            print(f"  - {sv['title'][:50]}... ({sv['reason']})")
        if len(skipped_videos) > 5:
            print(f"  ... ä»– {len(skipped_videos) - 5}ä»¶")

    # STEP 7: ç¿»è¨³
    print("\n" + "=" * 60)
    print("STEP 7: ç¿»è¨³")
    print("=" * 60)
    print("ç¿»è¨³ã¯ translator/claude_cli.py ã‚’ä½¿ã£ã¦åˆ¥é€”å®Ÿè¡Œã—ã¦ãã ã•ã„:")
    print("  cd /Users/thiroki34/poe-build-search")
    print("  python -m translator.claude_cli --all")

    # STEP 8: æ¤œè¨¼
    print("\n" + "=" * 60)
    print("STEP 8: æ¤œè¨¼")
    print("=" * 60)
    await validate_youtube_builds()

    print("\n" + "=" * 60)
    print("âœ… YouTubeã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å®Œäº†")
    print("=" * 60)


async def validate_youtube_builds():
    """YouTubeç”±æ¥ãƒ“ãƒ«ãƒ‰ã®æ¤œè¨¼"""
    import aiosqlite
    from app.config import settings

    db = await aiosqlite.connect(settings.db_path)
    try:
        # YouTubeç”±æ¥ãƒ“ãƒ«ãƒ‰ä»¶æ•°
        cursor = await db.execute(
            "SELECT COUNT(*) as cnt FROM builds WHERE source = 'youtube'"
        )
        row = await cursor.fetchone()
        youtube_count = row[0] if row else 0

        # ã‚´ãƒŸãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆdescription_enã«GARBAGE_PATTERNSãŒå«ã¾ã‚Œã‚‹ï¼‰
        from scraper.base import GARBAGE_PATTERNS
        garbage_count = 0
        for pattern in GARBAGE_PATTERNS:
            cursor = await db.execute(
                f"SELECT COUNT(*) as cnt FROM builds WHERE source = 'youtube' AND description_en LIKE '%{pattern}%'"
            )
            row = await cursor.fetchone()
            if row and row[0] > 0:
                garbage_count += row[0]

        # class_ja NULLï¼ˆYouTubeç”±æ¥ï¼‰
        cursor = await db.execute(
            "SELECT COUNT(*) as cnt FROM builds WHERE source = 'youtube' AND class_ja IS NULL"
        )
        row = await cursor.fetchone()
        class_ja_null = row[0] if row else 0

        print(f"  YouTubeç”±æ¥ãƒ“ãƒ«ãƒ‰ä»¶æ•°: {youtube_count}ä»¶")
        print(f"  ã‚´ãƒŸãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º: {garbage_count}ä»¶")
        print(f"  class_ja NULL: {class_ja_null}ä»¶")

        if garbage_count == 0 and class_ja_null == 0:
            print("  âœ… æ¤œè¨¼OK")
        else:
            print("  âš ï¸ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ã‚ã‚Š")

    finally:
        await db.close()


if __name__ == "__main__":
    asyncio.run(scrape_youtube_builds())
